{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo8RnaiRug7e"
      },
      "source": [
        "# **Modelo CNN Simple - ResNet18 + Dropout + Soft Attention Espacial** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kTmTkZCHywa2"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "y-4fhcf_yxMv"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),  # Redimensionar a un tamaño fijo (ej. 128x128)\n",
        "    transforms.ToTensor(),  # Convertir la imagen a un tensor de PyTorch\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalización (media y desviación estándar de imágenes ImageNet)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "u-v6MsjVzY6k"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (str): Ruta al archivo CSV con las imágenes y sus etiquetas.\n",
        "            img_dir (str): Ruta al directorio que contiene las imágenes.\n",
        "            transform (callable, optional): Transformaciones que se aplican a las imágenes.\n",
        "        \"\"\"\n",
        "        self.img_labels = pd.read_csv(csv_file)  # Leer el archivo CSV con las etiquetas\n",
        "        self.img_dir = img_dir  # Ruta donde están las imágenes\n",
        "        self.transform = transform  # Transformaciones a aplicar\n",
        "\n",
        "        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(self.img_labels['diagnosis'].unique())}\n",
        "    def __len__(self):\n",
        "        \"\"\"Retorna el número total de imágenes en el dataset\"\"\"\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Obtiene una imagen y su etiqueta\"\"\"\n",
        "        img_name = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])  # Nombre de la imagen\n",
        "        image = Image.open(img_name)  # Abrir la imagen\n",
        "        label = self.class_to_idx[self.img_labels.iloc[idx, 3]] # Etiqueta asociada\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)  # Aplicar transformaciones si es necesario\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Generación del dataset de entrenamiento y validación**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Leer el CSV\n",
        "csvRoute=\"bcn_20k_train.csv\"\n",
        "df = pd.read_csv(csvRoute)\n",
        "#Definir las clases que deseas excluir por su nombre\n",
        "clases_a_excluir = ['SCC', 'DF', 'VASC']  # Sustituye estos nombres por las clases que quieres excluir\n",
        "\n",
        "# Filtrar el DataFrame para excluir las clases especificadas\n",
        "df_filtrado = df[~df['diagnosis'].isin(clases_a_excluir)]\n",
        "\n",
        "df_filtrado.to_csv(\"bcn_20k_train_filtrado.csv\", index=False)\n",
        "\n",
        "# Dividir el dataset en entrenamiento (80%) y validación (20%)\n",
        "train_df, val_df = train_test_split(df_filtrado, test_size=0.2, random_state=42)\n",
        "\n",
        "# Crear el dataset de entrenamiento y validación\n",
        "train_dataset = CustomDataset(csv_file=\"bcn_20k_train_filtrado.csv\", img_dir='bcn_20k_train', transform=transform)\n",
        "val_dataset = CustomDataset(csv_file=\"bcn_20k_train_filtrado.csv\", img_dir='bcn_20k_train', transform=transform)\n",
        "\n",
        "# Actualizar los datasets con los subconjuntos correspondientes\n",
        "train_dataset.img_labels = train_df\n",
        "val_dataset.img_labels = val_df\n",
        "\n",
        "# Crear los DataLoader\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Guardado de los datasets divididos inicialmente**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_df.to_json('data_train_resnet18_softAtt.json', orient='records', lines=True)\n",
        "val_df.to_json('data_val_resnet18_softAtt.json', orient='records', lines=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Definición del Modelo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Mecanismo de Soft-Attention Espacial\n",
        "class SpatialAttention(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "        self.conv_attention = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 1, kernel_size=1),  # Mapa de atención 1x1\n",
        "            nn.Softmax(dim=2)                           # Normalización espacial\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x: (batch, 512, H, W) [Ej: (batch, 512, 7, 7)]\n",
        "        \n",
        "        # Generar mapa de atención (batch, 1, H, W)\n",
        "        attn_weights = self.conv_attention(x)\n",
        "        \n",
        "        # Aplicar atención: características * pesos\n",
        "        attended_features = x * attn_weights  # Broadcasting automático\n",
        "        \n",
        "        return attended_features\n",
        "\n",
        "\n",
        "# Modelo ResNet18 con Dropout + Soft-Attention\n",
        "class ResNet18WithAttention(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(ResNet18WithAttention, self).__init__()\n",
        "        \n",
        "        # 1. Cargar ResNet18 preentrenado\n",
        "        self.resnet18 = models.resnet18(pretrained=True)\n",
        "        \n",
        "        # 2. Congelar capas convolucionales\n",
        "        for param in self.resnet18.parameters():\n",
        "            param.requires_grad = False\n",
        "            \n",
        "        # 3. Añadir mecanismo de atención espacial\n",
        "        self.attention = SpatialAttention(in_channels=512)  # ResNet18 tiene 512 canales al final\n",
        "        \n",
        "        # 4. Modificar capas FC con Dropout\n",
        "        self.resnet18.fc = nn.Sequential(\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extraer características hasta la última capa convolucional\n",
        "        x = self.resnet18.conv1(x)\n",
        "        x = self.resnet18.bn1(x)\n",
        "        x = self.resnet18.relu(x)\n",
        "        x = self.resnet18.maxpool(x)\n",
        "        x = self.resnet18.layer1(x)\n",
        "        x = self.resnet18.layer2(x)\n",
        "        x = self.resnet18.layer3(x)\n",
        "        x = self.resnet18.layer4(x)  # Salida: (batch, 512, 7, 7)\n",
        "        \n",
        "        # Aplicar atención espacial\n",
        "        x = self.attention(x)  # (batch, 512, 7, 7) con pesos aprendidos\n",
        "        \n",
        "        # Global Average Pooling y clasificación\n",
        "        x = self.resnet18.avgpool(x)  # (batch, 512, 1, 1)\n",
        "        x = torch.flatten(x, 1)       # (batch, 512)\n",
        "        x = self.resnet18.fc(x)       # (batch, num_classes)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Entrenamiento y validación del modelo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verificar si CUDA (GPU) está disponible\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Definición de la cantidad de clases, la función de perdida, el optimizador y el learning rate estático\n",
        "modelAt = ResNet18WithAttention(num_classes=5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(modelAt.parameters(), lr=0.001)\n",
        "\n",
        "# Entrenar el modelo\n",
        "num_epochs = 10\n",
        "\n",
        "model = modelAt.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Modo entrenamiento\n",
        "    running_loss = 0.0\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "\n",
        "    # Entrenamiento\n",
        "    for inputs, labels in train_dataloader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Estadísticas de la pérdida\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Precisión\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct_preds += (predicted == labels).sum().item()\n",
        "        total_preds += labels.size(0)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_dataloader)}, Accuracy: {100 * correct_preds / total_preds}%\")\n",
        "    # Validación\n",
        "    model.eval()  # Modo evaluación\n",
        "    correct_preds = 0\n",
        "    total_preds = 0\n",
        "    with torch.no_grad():  # No calcular gradientes durante la validación\n",
        "        for inputs, labels in val_dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            correct_preds += (predicted == labels).sum().item()\n",
        "            total_preds += labels.size(0)\n",
        "\n",
        "    print(f\"Validation Accuracy: {100 * correct_preds / total_preds}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**70% de Accuracy de validación en 50 Epocas**\n",
        "\n",
        "**143 minutos 64.85% en validación 10 Epocas:**\n",
        "\n",
        "Epoch 1/10, Loss: 1.134643373440723, Accuracy: 56.09236990528892%\n",
        "Validation Accuracy: 59.91489361702128%\n",
        "Epoch 2/10, Loss: 1.0131697717572556, Accuracy: 61.57284239650952%\n",
        "Validation Accuracy: 60.5531914893617%\n",
        "Epoch 3/10, Loss: 0.9697256232200026, Accuracy: 63.2648717675854%\n",
        "Validation Accuracy: 62.723404255319146%\n",
        "Epoch 4/10, Loss: 0.959287223969998, Accuracy: 63.14781313185059%\n",
        "Validation Accuracy: 63.276595744680854%\n",
        "Epoch 5/10, Loss: 0.9335303259950106, Accuracy: 64.60572523145684%\n",
        "Validation Accuracy: 63.702127659574465%\n",
        "Epoch 6/10, Loss: 0.9142697725166269, Accuracy: 64.97818452697669%\n",
        "Validation Accuracy: 63.1063829787234%\n",
        "Epoch 7/10, Loss: 0.8985673685868582, Accuracy: 65.32936043418113%\n",
        "Validation Accuracy: 63.61702127659574%\n",
        "Epoch 8/10, Loss: 0.8820558255221568, Accuracy: 66.14877088432479%\n",
        "Validation Accuracy: 63.829787234042556%\n",
        "Epoch 9/10, Loss: 0.8775967738660825, Accuracy: 66.44673832074066%\n",
        "Validation Accuracy: 63.744680851063826%\n",
        "Epoch 10/10, Loss: 0.8590307535768367, Accuracy: 66.86176439289135%\n",
        "Validation Accuracy: 64.85106382978724%\n",
        "\n",
        "**142 minutos 66.9% en validación 10 Epocas (Primero en detectar KA):**\n",
        "\n",
        "Epoch 1/10, Loss: 0.8453486519200462, Accuracy: 67.73438331382356%\n",
        "Validation Accuracy: 65.23404255319149%\n",
        "Epoch 2/10, Loss: 0.8239534395892604, Accuracy: 68.34095988081303%\n",
        "Validation Accuracy: 65.02127659574468%\n",
        "Epoch 3/10, Loss: 0.8275354070728328, Accuracy: 68.11748430350112%\n",
        "Validation Accuracy: 66.25531914893617%\n",
        "Epoch 4/10, Loss: 0.8035658865558858, Accuracy: 68.57507715228265%\n",
        "Validation Accuracy: 65.40425531914893%\n",
        "Epoch 5/10, Loss: 0.7843043409845456, Accuracy: 70.22453974672769%\n",
        "Validation Accuracy: 65.48936170212765%\n",
        "Epoch 6/10, Loss: 0.7854251228019494, Accuracy: 69.91593061615409%\n",
        "Validation Accuracy: 66.59574468085107%\n",
        "Epoch 7/10, Loss: 0.7668884254637218, Accuracy: 70.21389805256997%\n",
        "Validation Accuracy: 65.82978723404256%\n",
        "Epoch 8/10, Loss: 0.7613350249269382, Accuracy: 70.67149090135149%\n",
        "Validation Accuracy: 66.34042553191489%\n",
        "Epoch 9/10, Loss: 0.7379224217262398, Accuracy: 72.19325316590401%\n",
        "Validation Accuracy: 67.19148936170212%\n",
        "Epoch 10/10, Loss: 0.7312207400393324, Accuracy: 71.50154304565287%\n",
        "Validation Accuracy: 66.93617021276596%\n",
        "\n",
        "**150 minutos 68.12% en validación 10 Epocas**\n",
        "\n",
        "Epoch 1/10, Loss: 0.7263397300729946, Accuracy: 72.32095349579653%\n",
        "Validation Accuracy: 66.68085106382979%\n",
        "Epoch 2/10, Loss: 0.7067413063479119, Accuracy: 72.42737043737363%\n",
        "Validation Accuracy: 67.23404255319149%\n",
        "Epoch 3/10, Loss: 0.701630963456063, Accuracy: 73.42768968819836%\n",
        "Validation Accuracy: 67.06382978723404%\n",
        "Epoch 4/10, Loss: 0.6938930058560404, Accuracy: 73.05523039267851%\n",
        "Validation Accuracy: 66.12765957446808%\n",
        "Epoch 5/10, Loss: 0.6726329312438056, Accuracy: 74.4386506331808%\n",
        "Validation Accuracy: 67.70212765957447%\n",
        "Epoch 6/10, Loss: 0.6607335396364432, Accuracy: 74.71533468128126%\n",
        "Validation Accuracy: 67.48936170212765%\n",
        "Epoch 7/10, Loss: 0.6529108140947056, Accuracy: 75.17292753006278%\n",
        "Validation Accuracy: 67.44680851063829%\n",
        "Epoch 8/10, Loss: 0.6382793583432023, Accuracy: 75.81142917952538%\n",
        "Validation Accuracy: 68.12765957446808%\n",
        "Epoch 9/10, Loss: 0.6398449084993933, Accuracy: 76.21581355751836%\n",
        "Validation Accuracy: 68.68085106382979%\n",
        "Epoch 10/10, Loss: 0.6278198312739937, Accuracy: 76.01362136852187%\n",
        "Validation Accuracy: 68.12765957446808%\n",
        "\n",
        "**130 minutos 68.7% en validación 10 Epocas**\n",
        "\n",
        "Epoch 1/10, Loss: 0.6140470188491198, Accuracy: 76.65212301798447%\n",
        "Validation Accuracy: 68.2127659574468%\n",
        "Epoch 2/10, Loss: 0.6087234322311116, Accuracy: 76.29030541662233%\n",
        "Validation Accuracy: 68.68085106382979%\n",
        "Epoch 3/10, Loss: 0.5855527020433322, Accuracy: 77.73757582207088%\n",
        "Validation Accuracy: 69.14893617021276%\n",
        "Epoch 4/10, Loss: 0.5734712509881883, Accuracy: 78.36543577737577%\n",
        "Validation Accuracy: 68.76595744680851%\n",
        "Epoch 5/10, Loss: 0.5858643731089677, Accuracy: 78.23773544748325%\n",
        "Validation Accuracy: 68.85106382978724%\n",
        "Epoch 6/10, Loss: 0.5681056010277092, Accuracy: 78.1951686708524%\n",
        "Validation Accuracy: 69.02127659574468%\n",
        "Epoch 7/10, Loss: 0.563230012549835, Accuracy: 78.833670320315%\n",
        "Validation Accuracy: 69.31914893617021%\n",
        "Epoch 8/10, Loss: 0.5470320342146621, Accuracy: 79.9084814302437%\n",
        "Validation Accuracy: 68.59574468085107%\n",
        "Epoch 9/10, Loss: 0.5348383441161947, Accuracy: 79.63179738214323%\n",
        "Validation Accuracy: 69.36170212765957%\n",
        "Epoch 10/10, Loss: 0.5381639800002785, Accuracy: 79.35511333404278%\n",
        "Validation Accuracy: 68.76595744680851%\n",
        "\n",
        "**113 minutos 70.29% en validación 10 Epocas (Ya Detecta KA y CCB)**\n",
        "\n",
        "Epoch 1/10, Loss: 0.5233964961605008, Accuracy: 80.05746514845163%\n",
        "Validation Accuracy: 69.23404255319149%\n",
        "Epoch 2/10, Loss: 0.5092849620953709, Accuracy: 80.0255400659785%\n",
        "Validation Accuracy: 69.48936170212765%\n",
        "Epoch 3/10, Loss: 0.5059386601253432, Accuracy: 80.62147493881025%\n",
        "Validation Accuracy: 68.17021276595744%\n",
        "Epoch 4/10, Loss: 0.5176538025238075, Accuracy: 79.86591465361286%\n",
        "Validation Accuracy: 68.8936170212766%\n",
        "Epoch 5/10, Loss: 0.4845995868043024, Accuracy: 81.61115249547728%\n",
        "Validation Accuracy: 69.61702127659575%\n",
        "Epoch 6/10, Loss: 0.497197156657978, Accuracy: 81.17484303501118%\n",
        "Validation Accuracy: 70.2127659574468%\n",
        "Epoch 7/10, Loss: 0.4828392481621431, Accuracy: 81.64307757795041%\n",
        "Validation Accuracy: 69.65957446808511%\n",
        "Epoch 8/10, Loss: 0.47273193527849355, Accuracy: 81.76013621368521%\n",
        "Validation Accuracy: 69.91489361702128%\n",
        "Epoch 9/10, Loss: 0.4651789545607405, Accuracy: 82.17516228583591%\n",
        "Validation Accuracy: 69.87234042553192%\n",
        "Epoch 10/10, Loss: 0.46274231151253187, Accuracy: 82.09002873257423%\n",
        "Validation Accuracy: 70.29787234042553%\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Guardado del modelo completo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "F8oSSsxF3tTB"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'modelo_entrenado_resnet18_softAtt_ka_completo_70_5_clases.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Guardado de los pesos del modelo**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'modelo_entrenado_resnet18_softAtt_ka_pesos_70_5_clases.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Evaluación en datos reales**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'MEL': 0, 'NV': 1, 'BCC': 2, 'BKL': 3, 'AK': 4}\n"
          ]
        }
      ],
      "source": [
        "# Mapeo de clases\n",
        "print(train_dataset.class_to_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n",
            "Predicción: AK\n"
          ]
        }
      ],
      "source": [
        "model.eval()  # Ponemos el modelo en modo de evaluación\n",
        "\n",
        "# Paso 3: Cargar la imagen y aplicar las transformaciones\n",
        "image_path = 'ka.jpg'  # Pon aquí la ruta de tu imagen\n",
        "image = Image.open(image_path)  # Abrir la imagen\n",
        "image_tensor = transform(image)  # Aplicar las transformaciones\n",
        "\n",
        "image_tensor = image_tensor.unsqueeze(0)  # Convertirlo a un batch de tamaño 1\n",
        "\n",
        "# Paso 5: Mover la imagen al dispositivo (GPU o CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "image_tensor = image_tensor.to(device)\n",
        "model = model.to(device)\n",
        "\n",
        "# Paso 6: Realizar la predicción\n",
        "with torch.no_grad():  # No necesitamos gradientes para la inferencia\n",
        "    output = model(image_tensor)\n",
        "\n",
        "# Paso 7: Convertir las predicciones en probabilidades con softmax\n",
        "probabilities = torch.nn.functional.softmax(output, dim=1)  # Usamos dim=1 porque tenemos un batch\n",
        "\n",
        "# Paso 8: Obtener la clase con la mayor probabilidad\n",
        "_, predicted_class = torch.max(probabilities, dim=1)\n",
        "\n",
        "# Paso 9: Interpretar la clase predicha\n",
        "# Usamos el mapeo que ya tienes de clases (el 'class_to_idx' que ya definiste en tu dataset)\n",
        "predicted_idx = predicted_class.item()  # Obtenemos el índice de la clase predicha\n",
        "print(predicted_idx)\n",
        "# Aquí usamos el mapeo de clases que creamos antes para convertir el índice a una clase legible\n",
        "predicted_class_name = [key for key, value in train_dataset.class_to_idx.items() if value == predicted_idx][0]\n",
        "\n",
        "# Mostrar la clase predicha\n",
        "print(f\"Predicción: {predicted_class_name}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deepLearningSkinDisVenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
